<a href="#computertalk">Can computers really talk</a><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<div align="center">1</div>
Language is the foundation of culture and technology. 
<div align="center"><b>
The uniquely human power <br>to implant complex thoughts into each other's minds,<br>
just by making sounds, gestures, or marks on paper.
</b></div><br>
It's a feat of such staggering sophistication that the world's most advanced supercomputers can't compete with the proficiency of a three-year-old child. We use this ability all day, every day, but rarely stop to think about how incredible it is. <br><br>
Behind every word and figure of speech is a story about where we came from and how we relate to each other.
Let's dive into what we say and why we say it -- an investigation that will span the fields of history, biology, pop culture, and more.<br><br><br><br>
<h1>Word Origins</h1>
Whenever we see some word, we directly map it to the meaning of it. Say when we hear the word "monster", we think of something huge or intimidating. But what most of us don’t think about is where the word came from. As far as our brains are concerned, word origins are irrelevant. All that matters is that you think of the same thing I do when I speak it.<br><br>
The study of the origins of words, known as <b>Etymology</b>, goes way deeper than just opening a dictionary. Just like living creatures, words don’t pop up fully-formed. They evolve over thousands of years, with ancestors that stretch back to the earliest Homo sapiens. Tracing the history of a word shows us how human ideas change over time, and reveals connections between cultures that would otherwise be forgotten.
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<div align="center">2</div>
So let’s follow this <b>monster’s</b> footprints into the prehistoric past, across multiple continents and several languages, to a place that only exists in theory...<br><br><br>
<h1>1. from French</h1>
<b>Anglo saxons</b> are the cultural group who inhabited England in the Early Middle Ages. In 1066, the <b>Norman Conquest</b> (invasion and occupation of England) was done by an army led by the Duke of Normandy (region of Northern France), later styled <b>William the Conqueror</b>. He installed a bunch of French nobles to rule it, they brought a lot of French words with them. This is one of the reasons why English, though technically a Germanic language, is filled with Latin words.<br><br>
<div align="center">
<img src="https://raw.githubusercontent.com/saivault/dsanotes/main/english/englishbreakdown.png" height="150" width="180">
</div><br>
Because the invaders were the new aristocracy (form of government that places strength in the hands of a small, privileged ruling class) in England, their fancy Latin words tended to be things related to government, academia, and technology. To this day, English words for simple or domestic things are usually Germanic in origin, like house, food and bread, while words that are more “intellectual” are usually from Latin, like representative, dissertation, and combustion.<br><br>
<div align="center"><b>
simple words - german<br>
intellectual words - latin&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</b>
</div><br>
This class distinction was so dramatic that when peasants raised livestock on their farms, they used the Anglo-Saxon words pig, sheep, and cow. But when the meat was served to the nobles, they switched to the French: pork, mutton, and beef.<br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<div align="center">3</div>
<h1>2. from Latin</h1>
Latin, the official language of ancient Rome.<br>
But how did France get words from Rome?<br><br>
You can thank <b>Julius Caesar</b> for that.  Around 55 B.C., he invaded what is now France, which at the time was occupied by a Celtic-speaking people known as the Gauls. Over time, a colloquial version of Latin called Vulgar Latin mixed with the Gallic language to create what we now know as French.<br>
<div align="center" style="margin-top:5px;margin-bottom:5px"><b>gallic + vulgar latin -> old french -> french</b></div>
In fact, all the <b>romance languages</b>, including Italian, Spanish and Portuguese, evolved from Vulgar Latin.<br><br><br><br>
<h2>Back to tracing origins</h2>
“Monster” entered the English lexicon as a version of the French <b>"monstre"</b>.<br>
Like most French words, monstre has a Latin root: <b>"monstrum"</b>, which meant evil omen. <br><br>
At that time, the appearance of strange creatures or animals with birth defects was considered a warning that something bad was about to happen.<br>It may seem silly today, but you can kind of see how, long before anyone understood genetic mutation or prenatal development, having a two-headed goat pop up on your farm might make you feel like someone was trying to tell you something -- and that something probably wasn’t good.<br><br><br>
Monstrum was derived from the Latin verb <b>monere</b>, which meant <b>“to warn, remind or instruct.</b>”
A lot of other English words are derived from monere, like premonition, admonish, demonstrate and monitor. When words share an original root like this, we call them cognates.<br><br>
<h3>Cognates</h3>
Cognate comes from the Latin for “born together,” essentially implying that these words are siblings. Incidentally, since they share a common parent, the words nation, nature and pregnancy are all cognates of cognate.
<br><br><br><br><br><br><br><br><br><br><br><br><br><div align="center">4</div>
Some people think that the Roman goddess <b>Juno Moneta</b> was also named after monere, because, as the protectress of the city’s funds, she “warned” against economic instability. Her name is where we get the words money and mint, so if true, we can add them to the list of monster’s cognates.<br><br><br>
It seems like the word monster has changed a lot since its origins in Ancient Rome, but maybe not as much as you’d think. Today, we don’t explicitly associate monsters with bad omens, but it is there if you look for it. 
<ul>
<li>What is Godzilla but a warning about the dangers of atomic power? </li>
<li>Mary Shelley used Frankenstein’s monster as a warning about scientific hubris</li>
<li>George Romero used zombies to warn us about social ills like racism, commercialism, and inequality.</li>
</ul><br><br>
Somehow, the notion of scary beasts being harm bringers of worse things to come is still somewhere in our psyche. And when you spend your life studying monsters, you start to see that <b>underneath the teeth and claws and fur are human fears about very real things.</b> Okay, so we’ve established that the scary, mythical beast we now call “monster,” was originally monere, a warning or a lesson.<br><br>
<div align="center">monster <- monstre <- monstrum <- monere</div>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(english)&nbsp;&nbsp;&nbsp;(french)&nbsp;&nbsp;&nbsp;&nbsp;(latin)&nbsp;&nbsp;&nbsp;(latin)<br><br>
But is that the end of the road? After all, monere didn’t come out of nowhere. <br>
It, too, must have ancestors. But to find them, we have to travel back before the written word, which means we don’t have much hard evidence, and have to rely instead on theory.<br><br><br>
<h2>The Grand Parent Language</h2>
In the late 18th century, the British philologist (a collector of words and their etymologies) <b>William Jones noticed striking similarities between seemingly unrelated languages</b> like English, Greek, Celtic and Sanskrit. The similarities were too frequent to be coincidental, and often followed similar patterns.<br><br>
<table style="padding-left:150px">
<tr><th>English</th><th>Latin</th><th>Greek</th><th>Celtic</th><th>Sanskrit</th></tr>
<tr><td>mother</td><td>mater</td><td>meter</td><td>mathir</td><td>matr</td></tr>
<tr><td>father</td><td>pater</td><td>pater</td><td>athir</td><td>pitr</td></tr>
<tr><td>two</td><td>duo</td><td>duo</td><td>da</td><td>dva</td></tr>
</table>
<br><br><br><br><br><br><div align="center">5</div>
He theorized that several of the major language groups were actually <b>descended from the same grandparent language, </b>which linguists today call <b>Proto-Indo-European</b>, or PIE. There are several theories about where and when PIE was spoken, but the most popular puts it just north of the Black Sea, about 4500 to 2500 B.C.<br><br><br>
As these ancient people spread throughout Europe and Asia, their dialects diverged and mixed with local populations to become early versions of several large language groups. Today, there are over 400 spoken languages
that descended from Proto-Indo-European, comprising 3.2 billion native speakers.<br><br><br>
To be clear, there is <b>no direct evidence of PIE</b>. It exists only in theory, though that theory is pretty widely accepted. Not a whole lot is known about its speakers, but if we look at which words modern Indo-European languages have in common, we can guess what kind of world they lived in… what food they ate, what terrain they inhabited, what technologies they used. And by tracing the pronunciations of words over time, linguists can even triangulate back to what PIE may have sounded like.<br><br><br>
<h2>Monsters and Mind</h2>
The Latin monere is most likely a derivative of the Proto-Indo-European root <b>men-, which meant “to think”</b>. There are thousands of words around the world that descend from this source, like the Russian mnit, the Sanskrit manayati, and plenty in English like mental, memory, and mania. Now maybe it feels like we’ve gone too far back in time to discover anything that would be relevant to us today. But I think it makes sense that <b>“monster” and “mind” are cognates</b>. These are creatures that spring from our imagination, inspired by fear or anger or guilt.<br><br><br>
Most popular <b>monsters of fiction are metaphors for something psychological</b>, whether it’s the sexual anxiety of Dracula, the split personality of Dr. Jekyll, or the grief of The Babadook. And when Jim Henson had to come up with a cast of muppets to help children understand their emotions, he chose monsters.<br><br><br>
Etymology doesn’t just show us how words evolve over time, but how ideas evolve. It’s almost like
tracing the thought process of human civilization. And though it’s taken hundreds of years for
linguists to track this "monster" all the way back to the shores of the Black Sea, in some ways,
the connection was there the whole time.<br><br><br>
<blockquote><b>
How many of us as small children insisted that there was a monster in our closet or under our bed, only to have a parent assure us, “It’s all in your mind.”</b></blockquote>
<br><br><br><br><br><div align="center">6</div>
<h1 id="computertalk" style="font-size:32px">Can computers really talk ?</h1>
<h2>Or are they faking it ?</h2>
In the mid-1960s a computer scientist named <b>Joseph Weisenbaum</b> created one of the <b>first natural
language processing programs, called ELIZA.</b> The user would type conversational sentences
and get responses in return -- a system that today we might call a <b>"chatbot"</b>, like the ones you find
on customer support websites. <br><br>
User: Hello<br>
Eliza: How are you today ?<br>
User: <u>I'm</u> okay<br>
Eliza: <u>Do you enjoy being</u> okay ?<br><br>
ELIZA used simple pattern matching and substitution to give the illusion of comprehension. She was designed with the personality of a psychotherapist, so that she could <b> reply with canned responses</b> (pre-recorded) , like "Why do you say that?" or "Is that important to you?"<br><br>
User: It's been hard lately<br>
Eliza: Can you elaborate on that ?<br><br>
Weisenbaum actually created ELIZA to demonstrate how shallow computer language skills were,
so imagine his surprise that many users actually believed she understood what they were saying.
I guess <b>it's in our nature to anthropomorphize things</b>. After all, I've been calling a computer
program "she". ELIZA was one of the first programs that could participate in the Turing Test.<br><br><br>
<h2>Turing test</h2>
A theoretical experiment proposed by computer scientist Alan Turing in 1950 as an assessment of artificial intelligence. An evaluator would eavesdrop on a typed conversation between two participants: a human and a computer. If the evaluator was unable to determine which was which, the computer was said to have passed the test.<br><br><br> 
Not long after ELIZA, another computer scientist named <b>Terry Winograd</b> developed his own natural
language processor called <b>SHRDLU</b>. Unlike ELIZA, SHRDLU was supposed to actually understand
what it was talking about -- so to speak. Its comprehension was <b>limited to a handful of simple
objects and actions</b>, but users could instruct it to rearrange the objects around a virtual room,
and then ask questions to verify that it knew what it had accomplished.<br><br><br><br><br>
<div align="center">7</div>
say the smallest block is purple block<br>
User: Put the smallest block on the blue block<br>
Eliza: Okay<br>
User: What is under the purple block ?<br>
Eliza: Blue block<br><br><br>
SHRDLU was one of the <b>first successful attempts to teach a computer grammar</b>, to put words into categories like nouns, verbs and prepositions. To have rules about how they could combine and assign meanings to various combinations. <br>
The technology was popularized in the text adventure video games of the 1980s, which allowed players to interact with the game world by typing simple verb-noun commands like "GET SWORD," "GO EAST," or "OPEN DOOR." Half the fun of these games was trying different combinations to see just how... extensive the vocabulary was<br><br><br>
<h3>Able to follow rules. Will that work always ?</h3>
But this technology did not advance as quickly as some hoped, because programmers soon realized that they had to <b>manually code all the rules</b> of English grammar, which are a <b>lot more complicated</b> than "open door." Even a simple rule like "add -ed to make a verb past tense" has enough exceptions like blow, hit etc.. to give a human ESL student a headache. Even a simple sentence like "It's raining," would confound a computer because it couldn't proceed without knowing what "it" is.<br><br>
Grammar is also dependent on the listener having at least some understanding of how the world works.<br>
<h3>Example - 1</h3>
 Consider these two sentences: <br>
1. Take the cap off the milk and pour it.  <br>
2. Take the sheet off the bed and fold it.<br>

These seem like pretty simple instructions, but that's only because we already know that milk pours and sheets fold. A computer without that prior knowledge would have no way of knowing what the "it's" refer to.<br><br>
<h3>Example - 2</h3>
And how about these two sentences: <br>
1. Sarah drew the dog with a pencil. <br>
2. Sarah drew the dog with a bone. <br>

Again, neither of these sentences would give a human much trouble. But a computer couldn't be sure of the meanings unless it already knew that dogs don't carry pencils, and that you can't draw with bones.<br><br><br><br><br><br>
<div align="center">8</div>
So just <b>to get a computer to understand basic grammar</b>,<br>
&nbsp;&nbsp;&nbsp;you'd have to manually encode it with <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- vast complex rules of syntax and a <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- broad understanding of how thousands of different objects interact with each &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;other.<br><br><br><br>
<h2>Using probabilities</h2>
It's no wonder that in the 1990s computer scientists kind of gave up on so-called symbolic language processing in favor of a new strategy: <b>statistical language processing</b>. Instead of trying to teach a computer the rules of language, they developed algorithms that could <b>analyze large bodies of text, look for patterns, and then make
guesses based on statistical probabilities</b>.<br><br>
For instance, when you ask Siri "What's the weather looking like today?" she doesn't bother parsing your grammar. She simply homes in on keywords and guesses what you're looking for based on how common the request is. She'd probably give you the same answer if you said "whether today" or even just "weather."<br><br><br>
Another application of this technology is predictive text, which is what your phone does when it tries to <b>guess which word you're going to type next</b>. For example, if you type the word "good" the algorithm knows from looking at thousands of pages of text that the most likely word to follow is "morning" followed by "day" and "job". It
doesn't know why or what those words mean--just that it's a statistical probability.<br><br>
Believe it or not, there was a time when some thought this was how human speech worked; That our brains
picked words one by one in order, deciding what word was most likely to follow the one before.<br><br><br>
20th century linguists like <b>Noam Chomsky</b> have shown that human grammar is far too complex
to be constructed this way. Take this sentence, for instance: <br><br>
<div align="center">
"<u>The fact that</u> you went for a walk this late at night without an<br> umbrella even after hearing the weather report on the radio this afternoon."<br><br>
</div>
You can probably tell that there's something missing. That's because the opening few words obligate the speaker to finish the sentence with a verb phrase, like "is unbelievable." Your brain subconsciously remembers this commitment, even though there are 23 words in between. Language is full of such grammatical promises, like either-ors or if-thens. A computer program that only considers one word at a time would never be able to fulfill them. 
<br><br><br><br><br><br><div align="center">9</div>
However, recent advancements in digital neural networks are raising expectations of what predictive text can achieve.<br><br><br>
<h1>GPT</h1>
In 2020, the artificial intelligence company <b>OpenAI</b> released a beta version of one of the <b>most sophisticated natural language processors ever created, called GPT-3</b>.<br><br>
How is GPT-3 different from previous NLPs? <br>
So unlike most AI systems which are designed for one use case, GPT-3 and our API provides a general purpose text in text out interface, and it allows users to try it on virtually any English language task.<br><br>
Say I have a piece of a legal document maybe an NDA and I would ask GPT-3 to summarize this legal document like a second grader and GPT-3 would then be able to provide a couple sentences actually compressing and making that legal document into a much more understandable piece of text. <br><br><br><br>
<h2>It's process</h2>
So our model actually doesn't have a goal or objective other than predicting the next word. Like most predictive text programs, GPT-3 is <b>trained by feeding it a large body of text for analysis, known as a corpus</b>. And GPT-3's corpus is enormous. Somewhere around 2 billion pages of text, taken from wikipedia, digital books, and
a vast swath of the web. It analyzes this text, using hundreds of billions of parameters looking
for probabilities. And because it does much of it unsupervised, even its programmers don't know
exactly what patterns it's finding in our human speech. <br><br>
When GPT-3 is asked to complete a prompt, it uses what it's learned to guess what should come next. But where your phone guesses words, <b>GPT-3 guesses "tokens"</b>: four character blocks of text including spaces and symbols.
We as humans, when we see a sentence we see a specific set of words with spaces in between, etc. When GPT-3 "sees," they actually are seeing tokens, which you can think of actually like a jigsaw puzzle -- that allows GPT-3 to process more text. It's really trying to predict what the next token is going to be in a sentence, based on all of the previous text it's seen before in that prompt.<br><br><br>
<h3>Is it solving our problems</h3>
- Because of its longer memory it's able to complete grammatical commitments like "the fact that..." <br>
- And thanks to its huge corpus, it actually does seem to know that dogs are more interested in bones than pencils.

<br><div align="center">10</div>
It can even apply grammatical rules to unfamiliar words. A famous experiment from the 1950s asked
children to complete the following prompt.<br><br>
<div style="padding-left:30px">
& This is a symbol <br>
&& Now there is another one. There are two of them. They are two ______ <br>
</div><br>
Even though they had never seen the word "symbol" before, the vast majority of children were able to correctly apply an "-s" to make it plural. Similarly, even though GPT-3 has probably never seen the word "abcdefgh"
in all its billions of pages of corpus, it still knows to add an "-s" if you have two of them.<br><br><br>
<h3>So can GPT-3 pass the Turing Test?</h3>
Uh, most people familiar with the traditional Turing Test would probably say it comes very close. It can actually start to feel like you really are interacting, you know, with that person. <br>
User: If sky is sea, what does that make birds ?<br>
GPT-3: If sky is sea, birds are fish. <br><br>
But the longer you talk with it and really begin to push it, you do come across some mistakes. And so that does kind of indicate that okay yeah this isn't human.<br>
User: Who was the president of United status in 1600 ?<br>
GPT-3: Queen Elizabeth 1 <br><br>
So ultimately, it's it's coming close but it's not quite there.<br><br><br><br>
Despite GPT-3's often impressive performance, there is a fundamental difference between human speech and what GPT-3 does. 
<blockquote><b>Humans don't learn language by memorizing likely orders of words, <br>but instead word categories.</b> (parts of speech)</blockquote>
Chomsky demonstrated this with a famously nonsensical sentence: "Colorless green ideas sleep furiously."
Even though all your life you've probably never heard any of these words follow the one before it,
you still know that the sentence is grammatically correct, because the order of the word categories
is correct. But GPT-3 does not make a distinction between form and content.
It doesn't care that "colorless green ideas" is a grammatically correct noun phrase.
Only that the likelihood of those tokens going together is very, very small. <br><br><br>
<br><br><br><br><br><br><br><br><div align="center">11</div>
<h2>Conclusion</h2>
Our brains seem to be hardwired for grammar, which ironically is closer to how the old SHRDLU program worked.<br><br> 
<div align="center"><b>
We have thoughts about the world that exist prior to and independent of language, <br>and we use grammar as the container to deliver those thoughts to others.</b>
</div><br><br><br>
For all its complexity, GPT-3 is still guessing one token at a time, based on statistical probabilities. It has no plan or goal, unless given one by a human. That's why the developers at OpenAI prefer to think of GPT-3 as a <b>writer's tool rather than a writer itself</b>.<br><br><br>
It's pretty astounding at faking what human speech sounds like, but humans are still the only computers that can create the thoughts behind the words.


